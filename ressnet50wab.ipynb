{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sulabh007/oral_cancer_detection_DAI/blob/main/ressnet50wab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "KSu07JIudPlw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QdTHyHCv63y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25c3bdd9-3f11-4f09-f1e2-7e974f5f1bf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo... installing it.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import wandb\n",
        "except:\n",
        "    !pip install wandb\n",
        "    import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "aDTefzpsLkLI",
        "outputId": "ebd5d4f5-9c9b-46ec-c6ec-a4ecf36e6455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.3-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.40.2-py2.py3-none-any.whl (257 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.7/257.7 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.41 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.40.2 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPRsd5CAumR8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import os\n",
        "from pathlib import Path\n",
        "import re\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import torch\n",
        "\n",
        "from PIL import Image\n",
        "from pandas import DataFrame\n",
        "from typing import Tuple, Dict, List\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\"\"\"\n",
        "Contains functions for training and testing a PyTorch model.\n",
        "\"\"\"\n",
        "from torchinfo import summary\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qhv28XQDuna7",
        "outputId": "f6fa74da-067e-47a4-bd38-3a10204198ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drive = '/content/drive/MyDrive/'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Directory"
      ],
      "metadata": {
        "id": "vRE2ykQ9dguH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vxH5-fRusqS"
      },
      "outputs": [],
      "source": [
        "data_path = Path(drive)\n",
        "image_path = data_path/'NDB-UFES An oral cancer and leukoplakia dataset composed of histopathological images and patient data'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0pmsCnduunS",
        "outputId": "694a28e3-79eb-4cde-abb1-09d3ec0469c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target directory: /content/drive/MyDrive/NDB-UFES An oral cancer and leukoplakia dataset composed of histopathological images and patient data/patch\n"
          ]
        }
      ],
      "source": [
        "# Setup path for target directory\n",
        "target_directory = image_path / 'patch'\n",
        "print(f\"Target directory: {target_directory}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAi3CZjzu7P2"
      },
      "outputs": [],
      "source": [
        "image_dir = target_directory / 'images'\n",
        "train_df = pd.read_csv(target_directory/'sabpatch_parsed_folders.csv')\n",
        "train_ds = train_df[['path','lesion']]\n",
        "test_df = pd.read_csv(target_directory/'sabpatch_parsed_test.csv')\n",
        "test_ds = test_df[['path','lesion']]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Global Parameters"
      ],
      "metadata": {
        "id": "oJ7tsiMqdLEN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS6AfaYNxLj3"
      },
      "outputs": [],
      "source": [
        "ALPHA = 0.0001 ## Learning Rate\n",
        "EPOCH = 50  ## Epochs\n",
        "BATCH_SIZE = 32\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "MEAN = [0.485, 0.456, 0.406]\n",
        "STD = [0.229, 0.224, 0.225]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Image Dataset"
      ],
      "metadata": {
        "id": "0MRlB7Hcdmk4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_XICcUJvMrW"
      },
      "outputs": [],
      "source": [
        "# Write a custom dataset class (inherits from torch.utils.data.Dataset)\n",
        "class ImageFolderCustom(Dataset):\n",
        "\n",
        "    # 1. Initialize with a targ_dir and transform (optional) parameter\n",
        "    def __init__(self,\n",
        "                 targ_dir: str,\n",
        "                 path_df: DataFrame,\n",
        "                 transform=None) -> None:\n",
        "\n",
        "        # Get all image paths, classes\n",
        "        self.img_df = path_df\n",
        "\n",
        "        # Set all images to proper path\n",
        "        self.img_df['path'] = self.check_path(targ_dir)\n",
        "\n",
        "        self.paths = list(self.img_df['path'])\n",
        "\n",
        "        # Setup transforms\n",
        "        self.transform = transform\n",
        "\n",
        "        self.classes, self.class_to_idx = self.find_classes()\n",
        "\n",
        "    # 2. check if its already in proper format\n",
        "    def check_path(self,\n",
        "                   targ_dir: str) -> DataFrame:\n",
        "        if str(targ_dir) in self.img_df.iloc[0,0]:\n",
        "            return self.img_df['path'].astype('string')\n",
        "        else:\n",
        "            return str(targ_dir)+ '/' +  self.img_df['path'].astype('string')\n",
        "\n",
        "    # 3. Make function to load images\n",
        "    def load_image(self,\n",
        "                   index: int) -> Image.Image:\n",
        "        \"Opens an image via a path and returns it.\"\n",
        "        image_path = self.img_df.iloc[index, 0]\n",
        "        return Image.open(image_path)\n",
        "\n",
        "    # 4. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n",
        "    def __len__(self) -> int:\n",
        "        \"Returns the total number of samples.\"\n",
        "        return self.img_df.shape[0]\n",
        "\n",
        "    # 5. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n",
        "    def __getitem__(self,\n",
        "                    index: int) -> Tuple[torch.Tensor, int]:\n",
        "        \"Returns one sample of data, data and label (X, y).\"\n",
        "        img = self.load_image(index)\n",
        "        class_name  = self.img_df.iloc[index, 1] # expects path in data_folder/class_name/image.jpeg\n",
        "        class_idx = self.class_to_idx[class_name]\n",
        "\n",
        "        # Transform if necessary\n",
        "        if self.transform:\n",
        "            return self.transform(img), class_idx # return data, label (X, y)\n",
        "        else:\n",
        "            return img, class_idx # return data, label (X, y)\n",
        "\n",
        "\n",
        "    def find_classes(self) -> Tuple[List[str], Dict[str, int]]:\n",
        "\n",
        "        col = self.img_df.columns\n",
        "        # 1. Get the class names by scanning the target directory\n",
        "        classes = sorted(self.img_df[col[1]].unique())\n",
        "\n",
        "        # 2. Raise an error if class names not found\n",
        "        if not classes:\n",
        "            raise FileNotFoundError(f\"Couldn't find any classes.\")\n",
        "\n",
        "        # 3. Create a dictionary of index labels (computers prefer numerical rather than string labels)\n",
        "        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
        "        return classes, class_to_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5tL_xzovTBL"
      },
      "outputs": [],
      "source": [
        "def create_dataloader(dir,\n",
        "                      train_ds: DataFrame,\n",
        "                      test_ds: DataFrame) -> Tuple[DataLoader[float], DataLoader[float], List[str]]:\n",
        "\n",
        "\n",
        "    # train_transforms = transforms.Compose([\n",
        "    #     #transforms.Resize((64, 64)),\n",
        "    #     transforms.RandomHorizontalFlip(p=0.5),\n",
        "    #     #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    #     transforms.ToTensor()\n",
        "    # ])\n",
        "\n",
        "\n",
        "    manual_transforms = transforms.Compose([\n",
        "        transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
        "        transforms.ToTensor(), # 2. Turn image values to between 0 & 1\n",
        "        transforms.Normalize(mean = MEAN, # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n",
        "                         std = STD)\n",
        "    ])\n",
        "\n",
        "\n",
        "\n",
        "    # test_transforms = transforms.Compose([\n",
        "    #     #transforms.Resize((64, 64)),\n",
        "    #     transforms.ToTensor(),\n",
        "    # ])\n",
        "\n",
        "\n",
        "    train_data = ImageFolderCustom(targ_dir = dir,\n",
        "                                          path_df = train_ds,\n",
        "                                          transform= manual_transforms)\n",
        "\n",
        "    test_data = ImageFolderCustom(targ_dir = dir,\n",
        "                                          path_df = test_ds,\n",
        "                                          transform= manual_transforms)\n",
        "\n",
        "\n",
        "\n",
        "    classes, class_to_idx = train_data.find_classes()\n",
        "\n",
        "    # Turn train, test and eval Dataset's into DataLoader's\n",
        "\n",
        "    train_dataloader = DataLoader(dataset=train_data, # use custom created train Dataset\n",
        "                                        batch_size=BATCH_SIZE, # how many samples per batch?\n",
        "                                        num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n",
        "                                        shuffle=True) # shuffle the data?\n",
        "\n",
        "    test_dataloader = DataLoader(dataset=test_data, # use custom created test Dataset\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        num_workers=0,\n",
        "                                        shuffle=False) # don't usually need to shuffle testing data\n",
        "\n",
        "    return train_dataloader, test_dataloader, classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XBnwV8SvTsE",
        "outputId": "cba78ad0-ada9-4122-8d79-45ad868b3379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-b25ec1bd1d1a>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.img_df['path'] = self.check_path(targ_dir)\n"
          ]
        }
      ],
      "source": [
        "train_dataloader, test_dataloader, classes = create_dataloader(dir=image_dir, train_ds=train_ds, test_ds=test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lW_-UN86vvF0",
        "outputId": "e39c9ec1-23dd-4970-84d5-efeb37d6ceae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7ff1cdc2a770>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7ff1cdc29480>,\n",
              " ['OSCC', 'With dysplasia', 'Without dysplasia'])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "train_dataloader, test_dataloader, classes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Class"
      ],
      "metadata": {
        "id": "O1Ou6ivcd-dA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfT6he19vyKJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Prt2buBolpIl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "5d8731d6-d330-49ae-c64a-dd234e3913ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjhashulabh\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240209_073443-uzel8is6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jhashulabh/ressnet50-v1/runs/uzel8is6' target=\"_blank\">polar-rain-1</a></strong> to <a href='https://wandb.ai/jhashulabh/ressnet50-v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jhashulabh/ressnet50-v1' target=\"_blank\">https://wandb.ai/jhashulabh/ressnet50-v1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jhashulabh/ressnet50-v1/runs/uzel8is6' target=\"_blank\">https://wandb.ai/jhashulabh/ressnet50-v1/runs/uzel8is6</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "class Engine():\n",
        "\n",
        "    wandb.init(\n",
        "    project=\"ressnet50-v1\",\n",
        "    config={\n",
        "            \"epochs\": EPOCH,\n",
        "            \"batch_size\": BATCH_SIZE,\n",
        "            \"lr\": ALPHA,\n",
        "            \"architecture\": \"CNN\",\n",
        "            })\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 loss_fn,\n",
        "                 optimizer,\n",
        "                 device,\n",
        "                 early_stopping = False):\n",
        "\n",
        "        self.model = model\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "\n",
        "        self.early_stopping = early_stopping\n",
        "        self.counter = 0\n",
        "        self.early_stop = False # type: ignore\n",
        "        self.best_score = None\n",
        "\n",
        "        # Copy your config\n",
        "        self.config = wandb.config\n",
        "\n",
        "    def model_summary(self,\n",
        "                      input_size,\n",
        "                      col_width = 20,):\n",
        "\n",
        "        return summary(self.model,\n",
        "                input_size = input_size, # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n",
        "                col_names = [\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "                col_width = col_width,\n",
        "                row_settings = [\"var_names\"])\n",
        "\n",
        "    def check_early_stop(self,\n",
        "                   val_loss,\n",
        "                   delta,\n",
        "                   verbose,\n",
        "                   patience):\n",
        "\n",
        "        score = -val_loss\n",
        "        # print(verbose)\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "\n",
        "        elif score < self.best_score + delta:\n",
        "            self.counter += 1\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Early stopping counter: {self.counter} out of {patience}\")\n",
        "\n",
        "            if self.counter >= patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.early_stop = False\n",
        "            self.counter = 0\n",
        "\n",
        "\n",
        "    def train_step(self, epoch):\n",
        "\n",
        "        y_tr_true, y_tr_pred= [], []\n",
        "        # Put model in train mode\n",
        "        self.model.train()\n",
        "\n",
        "        # Setup train loss and train accuracy values\n",
        "        train_loss, train_acc = 0, 0\n",
        "\n",
        "        n_steps_per_epoch = math.ceil(len(self.train_dataloader) / self.config.batch_size)\n",
        "\n",
        "        example_ct = 0\n",
        "        # Loop through data loader data batches\n",
        "        for i, data in enumerate(tqdm(self.train_dataloader, desc=f'Epoch {epoch + 1}/{self.epochs}', unit='batch')):\n",
        "            # Send data to target device\n",
        "            X_train, y_train = data\n",
        "\n",
        "            y_tr_true.extend(y_train) # collect all training labels\n",
        "\n",
        "            X_train = X_train.to(self.device)\n",
        "            y_train = y_train.to(self.device)\n",
        "\n",
        "            # 1. Forward pass\n",
        "\n",
        "            outputs = self.model(X_train)\n",
        "            _, y_pred = torch.max(outputs, 1) # max along an axis gives both max val as well as index\n",
        "            y_tr_pred.extend(y_pred.data.cpu())\n",
        "\n",
        "            # 2. Calculate  and accumulate loss\n",
        "            loss = self.loss_fn(outputs, y_train)\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # 3. Optimizer zero grad\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # 4. Loss backward\n",
        "            loss.backward()\n",
        "\n",
        "            # 5. Optimizer step\n",
        "            self.optimizer.step()\n",
        "\n",
        "\n",
        "        # Adjust metrics to get average loss and accuracy per batch\n",
        "        train_loss /= len(self.train_dataloader)\n",
        "        train_acc = accuracy_score(y_tr_true, y_tr_pred)\n",
        "\n",
        "\n",
        "\n",
        "        # Log Training info in W&B\n",
        "        metrics ={\n",
        "                      \"train/train_loss\": train_loss,\n",
        "                      \"train/epoch\": (i + 1 + (n_steps_per_epoch * epoch)) / n_steps_per_epoch,\n",
        "                      \"train/train_acc\": train_acc,\n",
        "                  }\n",
        "\n",
        "        wandb.log(metrics)\n",
        "\n",
        "        return train_loss, train_acc\n",
        "\n",
        "\n",
        "    def test_step(self, epoch):\n",
        "\n",
        "        y_ts_true,  y_ts_pred = [], []\n",
        "\n",
        "        # Put model in eval mode\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        # Setup test loss and test accuracy values\n",
        "        test_loss, test_acc = 0, 0\n",
        "\n",
        "        # Turn on inference context manager\n",
        "        with torch.inference_mode():\n",
        "            # Loop through DataLoader batches\n",
        "            for i, data in enumerate(tqdm(self.test_dataloader, desc=f'Epoch {epoch + 1}/{self.epochs}', unit='batch')):\n",
        "                # Send data to target device\n",
        "                X_test, y_test = data\n",
        "                y_ts_true.extend(y_test) # collect all training labels\n",
        "\n",
        "                y_test = y_test.to(self.device)\n",
        "                X_test = X_test.to(self.device)\n",
        "\n",
        "                # 1. Forward pass\n",
        "                outputs = self.model(X_test)\n",
        "                _, y_pred = torch.max(outputs, 1) # max along an axis gives both max val as well as index\n",
        "\n",
        "                y_ts_pred.extend(y_pred.data.cpu())\n",
        "\n",
        "                # 2. Calculate and accumulate loss\n",
        "                loss = self.loss_fn(outputs, y_test)\n",
        "                test_loss += loss.item()\n",
        "\n",
        "                # Calculate and accumulate accuracy\n",
        "\n",
        "\n",
        "        # Adjust metrics to get average loss and accuracy per batch\n",
        "        test_loss/= len(self.test_dataloader)\n",
        "        test_acc = accuracy_score(y_ts_true, y_ts_pred)\n",
        "\n",
        "\n",
        "\n",
        "        # Log Testing info in W&B\n",
        "        metrics ={\n",
        "                  \"test/test_loss\": test_loss,\n",
        "                  \"test/test_acc\": test_acc,\n",
        "                  }\n",
        "\n",
        "        wandb.log(metrics)\n",
        "\n",
        "        return test_loss, test_acc\n",
        "\n",
        "\n",
        "    def train(self,\n",
        "              train_dataloader,\n",
        "              test_dataloader,\n",
        "              epochs=1,\n",
        "              delta = 0,\n",
        "              patience = 10,\n",
        "              verbose = False):\n",
        "\n",
        "        self.epochs = epochs\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.test_dataloader = test_dataloader\n",
        "\n",
        "\n",
        "        # Create empty results dictionary\n",
        "        results = {\"epoch\":[],\n",
        "                \"train_loss\": [],\n",
        "                \"train_acc\": [],\n",
        "                \"test_loss\": [],\n",
        "                \"test_acc\": []\n",
        "        }\n",
        "\n",
        "        # Make sure model on target device\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Loop through training and testing steps for a number of epochs\n",
        "        for epoch in range(self.epochs):\n",
        "            train_loss, train_acc = self.train_step(epoch)\n",
        "            test_loss, test_acc = self.test_step(epoch)\n",
        "\n",
        "            # Print out what's happening\n",
        "            print(\n",
        "            f\"Epoch: {epoch+1} | \"\n",
        "            f\"train_loss: {train_loss:.4f} | \"\n",
        "            f\"train_acc: {train_acc:.4f} | \"\n",
        "            f\"test_loss: {test_loss:.4f} | \"\n",
        "            f\"test_acc: {test_acc:.4f}\"\n",
        "            )\n",
        "\n",
        "            # Update results dictionary\n",
        "            results[\"epoch\"].append(epoch+1)\n",
        "            results[\"train_loss\"].append(train_loss)\n",
        "            results[\"test_loss\"].append(test_loss)\n",
        "            results[\"train_acc\"].append(train_acc)\n",
        "            results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "\n",
        "\n",
        "            if self.early_stopping:\n",
        "                self.check_early_stop(test_loss, delta, verbose, patience)\n",
        "                if self.early_stop:\n",
        "                    print(\"Early Stopping\")\n",
        "                    break\n",
        "\n",
        "        # Mark the run as finished\n",
        "        wandb.finish()\n",
        "        # Return the filled results at the end of the epochs\n",
        "        return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "BSiK4ilqeGAV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5sq4qsrxKaM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8d853ce-3a96-4e53-ef01-fc03727c668d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 99.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "model = models.resnet50(pretrained=True).to(device)\n",
        "\n",
        "# for param in model.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "model.fc = nn.Sequential(\n",
        "               nn.Linear(2048, 128),\n",
        "               nn.ReLU(inplace=True),\n",
        "               nn.Linear(128, 3)).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5K1mCEDwI3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "2b116b0b61d14d939fdce3f78b2789cb"
          ]
        },
        "outputId": "6ff414b1-3357-44f2-c2ff-49340d170fb8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1/50:   0%|          | 0/98 [00:00<?, ?batch/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b116b0b61d14d939fdce3f78b2789cb"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss(reduction='sum') # computes the cross entropy loss between input logits and target.\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = ALPHA)\n",
        "\n",
        "# Start the timer\n",
        "from timeit import default_timer as timer\n",
        "start_time = timer()\n",
        "\n",
        "# Setup training and save the results\n",
        "Engine = Engine(model=model, loss_fn=loss_fn, optimizer=optimizer, device=device, early_stopping=True)\n",
        "result = Engine.train(train_dataloader=train_dataloader, test_dataloader=test_dataloader, epochs=EPOCH, verbose=True)\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(model), type(loss_fn), type(optimizer)"
      ],
      "metadata": {
        "id": "IqNAk_aQFGox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbrcqB0M0ozm"
      },
      "outputs": [],
      "source": [
        "result_df = pd.DataFrame(result)\n",
        "result_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df.shape"
      ],
      "metadata": {
        "id": "mPxdeX4o_yGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # parameters for Matplotlib\n",
        "# params = {'legend.fontsize': 'x-large',\n",
        "#           'figure.figsize': (15, 10),\n",
        "#           'axes.labelsize': 'x-large',\n",
        "#           'axes.titlesize':'x-large',\n",
        "#           'xtick.labelsize':'x-large',\n",
        "#           'ytick.labelsize':'x-large'\n",
        "#          }\n",
        "\n",
        "# CMAP = plt.cm.coolwarm\n",
        "\n",
        "# plt.rcParams.update(params)"
      ],
      "metadata": {
        "id": "yu7XVUxU_qiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot hist"
      ],
      "metadata": {
        "id": "AXtwVciAeMFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###-----------------------------------\n",
        "### Function to plot Loss Curve\n",
        "###-----------------------------------\n",
        "\n",
        "def plot_torch_hist(hist_df : pd.DataFrame):\n",
        "    '''\n",
        "    Args:\n",
        "      hist_df : pandas Dataframe with five columns\n",
        "                First column need to be epoch, 'x' values\n",
        "    '''\n",
        "    # instantiate figure\n",
        "    fig, axes = plt.subplots(1,2 , figsize = (15,6))\n",
        "\n",
        "    facecolor = 'cyan'\n",
        "    fontsize=12\n",
        "\n",
        "    # Get columns by index to eliminate any column naming error\n",
        "    x = \"epoch\"\n",
        "    y1 = \"train_loss\"\n",
        "    y2 = \"test_loss\"\n",
        "    y3 = \"train_acc\"\n",
        "    y4 = \"test_acc\"\n",
        "\n",
        "\n",
        "    # properties  matplotlib.patch.Patch\n",
        "    props = dict(boxstyle='round', facecolor=facecolor, alpha=0.5)\n",
        "\n",
        "    # Where was min loss\n",
        "    best = hist_df[hist_df[y2] == hist_df[y2].min()]\n",
        "\n",
        "    # pick first axis\n",
        "    ax = axes[0]\n",
        "\n",
        "    # Plot all losses\n",
        "    hist_df.plot(x = x, y = [y1,y2], ax = ax)\n",
        "\n",
        "    # little beautification\n",
        "    txtFmt = \"Loss: \\n  train: {:6.4f}\\n   test: {:6.4f}\"\n",
        "    txtstr = txtFmt.format(hist_df.iloc[-1][y1],\n",
        "                           hist_df.iloc[-1][y2]) #text to plot\n",
        "\n",
        "    # place a text box in upper middle in axes coords\n",
        "    ax.text(0.3, 0.95, txtstr,\n",
        "            transform=ax.transAxes,\n",
        "            fontsize=fontsize,\n",
        "            verticalalignment='top',\n",
        "            bbox=props)\n",
        "\n",
        "    # Mark arrow at lowest\n",
        "    offset = (best[y2].max() - best[y2].max())/10\n",
        "    ax.annotate(f'Min: {best[y2].to_numpy()[0]:6.4f}', # text to print\n",
        "                xy=(best[x].to_numpy(), best[y2].to_numpy()[0]), # Arrow start\n",
        "                xytext=(best[x].to_numpy()-2, best[y2].to_numpy()[0]+offset), # location of text\n",
        "                fontsize=fontsize, va='bottom', ha='right',bbox=props, # beautification of text\n",
        "                arrowprops=dict(facecolor=facecolor, shrink=0.05)) # arrow\n",
        "\n",
        "    # Draw vertical line at best value\n",
        "    ax.axvline(x = best[x].to_numpy(),\n",
        "               color = 'green',\n",
        "               linestyle='-.', lw = 3);\n",
        "\n",
        "    ax.set_xlabel(x.capitalize())\n",
        "    ax.set_ylabel(y1.capitalize())\n",
        "    ax.set_title('Errors')\n",
        "    ax.grid()\n",
        "    ax.legend(loc = 'upper left') # model legend to upper left\n",
        "\n",
        "    # pick second axis\n",
        "    ax = axes[1]\n",
        "\n",
        "    # Plot accuracy\n",
        "    hist_df.plot(x = x, y = [y3, y4], ax = ax)\n",
        "\n",
        "    # little beautification\n",
        "    txtFmt = \"Accuracy: \\n  train: {:6.4f}\\n  test:  {:6.4f}\"\n",
        "    txtstr = txtFmt.format(hist_df.iloc[-1][y3],\n",
        "                           hist_df.iloc[-1][y4]) #text to plot\n",
        "\n",
        "    # place a text box in lower middle in axes coords\n",
        "    ax.text(0.3, 0.2, txtstr,\n",
        "            transform=ax.transAxes, fontsize=fontsize,\n",
        "            verticalalignment='top', bbox=props)\n",
        "\n",
        "    # Mark arrow at lowest\n",
        "    offset = (best[y4].max() - best[y4].min())/10\n",
        "    ax.annotate(f'Best: {best[y4].to_numpy()[0]:6.4f}', # text to print\n",
        "                xy=(best[x].to_numpy(), best[y4].to_numpy()[0]), # Arrow start\n",
        "                xytext=(best[x].to_numpy()-2, best[y4].to_numpy()[0]-offset), # location of text\n",
        "                fontsize=fontsize, va='bottom', ha='right',bbox=props, # beautification of text\n",
        "                arrowprops=dict(facecolor=facecolor, shrink=0.05)) # arrow\n",
        "\n",
        "\n",
        "    # Draw a vertical line at best value\n",
        "    ax.axvline(x = best[x].to_numpy(),\n",
        "               color = 'green',\n",
        "               linestyle='-.', lw = 3)\n",
        "\n",
        "    # Labels\n",
        "    ax.set_xlabel(x.capitalize())\n",
        "    ax.set_ylabel(y3.capitalize())\n",
        "    ax.set_title('Accuracies')\n",
        "    ax.grid();\n",
        "    ax.legend(loc = 'lower left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # pick first axis\n",
        "    ax = axes[0]\n",
        "\n",
        "    # Plot all losses\n",
        "    hist_df.plot(x = x, y = [y1,y2], ax = ax)\n",
        "\n",
        "    # little beautification\n",
        "    txtFmt = \"Loss: \\n  train: {:6.4f}\\n   test: {:6.4f}\"\n",
        "    txtstr = txtFmt.format(hist_df.iloc[-1][y1],\n",
        "                           hist_df.iloc[-1][y2]) #text to plot\n",
        "\n",
        "    # place a text box in upper middle in axes coords\n",
        "    ax.text(0.3, 0.95, txtstr,\n",
        "            transform=ax.transAxes,\n",
        "            fontsize=fontsize,\n",
        "            verticalalignment='top',\n",
        "            bbox=props)\n",
        "\n",
        "    # Mark arrow at lowest\n",
        "    offset = (best[y2].max() - best[y2].max())/10\n",
        "    ax.annotate(f'Min: {best[y2].to_numpy()[0]:6.4f}', # text to print\n",
        "                xy=(best[x].to_numpy(), best[y2].to_numpy()[0]), # Arrow start\n",
        "                xytext=(best[x].to_numpy()-2, best[y2].to_numpy()[0]+offset), # location of text\n",
        "                fontsize=fontsize, va='bottom', ha='right',bbox=props, # beautification of text\n",
        "                arrowprops=dict(facecolor=facecolor, shrink=0.05)) # arrow\n",
        "\n",
        "    # Draw vertical line at best value\n",
        "    ax.axvline(x = best[x].to_numpy(),\n",
        "               color = 'green',\n",
        "               linestyle='-.', lw = 3);\n",
        "\n",
        "    ax.set_xlabel(x.capitalize())\n",
        "    ax.set_ylabel(y1.capitalize())\n",
        "    ax.set_title('Errors')\n",
        "    ax.grid()\n",
        "    ax.legend(loc = 'upper left') # model legend to upper left\n",
        "\n",
        "    # pick second axis\n",
        "    ax = axes[1]\n",
        "\n",
        "    # Plot accuracy\n",
        "    hist_df.plot(x = x, y = [y3, y4], ax = ax)\n",
        "\n",
        "    # little beautification\n",
        "    txtFmt = \"Accuracy: \\n  train: {:6.4f}\\n  test:  {:6.4f}\"\n",
        "    txtstr = txtFmt.format(hist_df.iloc[-1][y3],\n",
        "                           hist_df.iloc[-1][y4]) #text to plot\n",
        "\n",
        "    # place a text box in lower middle in axes coords\n",
        "    ax.text(0.3, 0.2, txtstr,\n",
        "            transform=ax.transAxes, fontsize=fontsize,\n",
        "            verticalalignment='top', bbox=props)\n",
        "\n",
        "    # Mark arrow at lowest\n",
        "    offset = (best[y4].max() - best[y4].min())/10\n",
        "    ax.annotate(f'Best: {best[y4].to_numpy()[0]:6.4f}', # text to print\n",
        "                xy=(best[x].to_numpy(), best[y4].to_numpy()[0]), # Arrow start\n",
        "                xytext=(best[x].to_numpy()-2, best[y4].to_numpy()[0]-offset), # location of text\n",
        "                fontsize=fontsize, va='bottom', ha='right',bbox=props, # beautification of text\n",
        "                arrowprops=dict(facecolor=facecolor, shrink=0.05)) # arrow\n",
        "\n",
        "\n",
        "    # Draw a vertical line at best value\n",
        "    ax.axvline(x = best[x].to_numpy(),\n",
        "               color = 'green',\n",
        "               linestyle='-.', lw = 3)\n",
        "\n",
        "    # Labels\n",
        "    ax.set_xlabel(x.capitalize())\n",
        "    ax.set_ylabel(y3.capitalize())\n",
        "    ax.set_title('Accuracies')\n",
        "    ax.grid();\n",
        "    ax.legend(loc = 'lower left')\n",
        "\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "YwNNRNpw-ZSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_torch_hist(result_df)"
      ],
      "metadata": {
        "id": "goNb0XL69RRQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}