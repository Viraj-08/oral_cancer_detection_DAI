{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7599778,"sourceType":"datasetVersion","datasetId":4423968}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"try:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary","metadata":{"execution":{"iopub.status.busy":"2024-02-11T14:27:22.847480Z","iopub.execute_input":"2024-02-11T14:27:22.847791Z","iopub.status.idle":"2024-02-11T14:27:26.261612Z","shell.execute_reply.started":"2024-02-11T14:27:22.847764Z","shell.execute_reply":"2024-02-11T14:27:26.260669Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport os\nfrom pathlib import Path\n\nimport re\nimport random\nimport matplotlib.pyplot as plt\nimport math\nimport torch\n\nfrom PIL import Image\nfrom pandas import DataFrame\nfrom typing import Tuple, Dict, List\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\n\nfrom sklearn.model_selection import StratifiedKFold\n\n\"\"\"\nContains functions for training and testing a PyTorch model.\n\"\"\"\nfrom torchinfo import summary\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n\nfrom torchvision import datasets, models, transforms\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.optim as optim","metadata":{"execution":{"iopub.status.busy":"2024-02-11T14:27:26.263671Z","iopub.execute_input":"2024-02-11T14:27:26.264247Z","iopub.status.idle":"2024-02-11T14:27:30.726651Z","shell.execute_reply.started":"2024-02-11T14:27:26.264213Z","shell.execute_reply":"2024-02-11T14:27:30.725701Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data_path = Path('/kaggle/input/oral-dataset/patches')\ndir_list = os.listdir(data_path)\nprint(\"Files and directories in '\", data_path, \"' :\")\n# prints all files\nprint(dir_list)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T14:27:30.727969Z","iopub.execute_input":"2024-02-11T14:27:30.728595Z","iopub.status.idle":"2024-02-11T14:27:30.742611Z","shell.execute_reply.started":"2024-02-11T14:27:30.728568Z","shell.execute_reply":"2024-02-11T14:27:30.741677Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Files and directories in ' /kaggle/input/oral-dataset/patches ' :\n['sabpatch_parsed_test.csv', 'images', 'sabpatch_parsed_folders.csv']\n","output_type":"stream"}]},{"cell_type":"code","source":"image_dir = data_path / 'images'\ntrain_df = pd.read_csv(data_path/'sabpatch_parsed_folders.csv')\ntrain_ds = train_df[['path','lesion']]\ntest_df = pd.read_csv(data_path/'sabpatch_parsed_test.csv')\ntest_ds = test_df[['path','lesion']]","metadata":{"execution":{"iopub.status.busy":"2024-02-11T14:27:30.744901Z","iopub.execute_input":"2024-02-11T14:27:30.745171Z","iopub.status.idle":"2024-02-11T14:27:30.770148Z","shell.execute_reply.started":"2024-02-11T14:27:30.745147Z","shell.execute_reply":"2024-02-11T14:27:30.769391Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"ALPHA = 0.00005 ## Learning Rate\nEPOCH = 10  ## Epochs\nBATCH_SIZE = 32\nK_FOLDS = 5\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nMEAN = [0.485, 0.456, 0.406]\nSTD = [0.229, 0.224, 0.225]","metadata":{"execution":{"iopub.status.busy":"2024-02-11T14:27:30.771064Z","iopub.execute_input":"2024-02-11T14:27:30.771313Z","iopub.status.idle":"2024-02-11T14:27:30.803083Z","shell.execute_reply.started":"2024-02-11T14:27:30.771291Z","shell.execute_reply":"2024-02-11T14:27:30.801938Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Write a custom dataset class (inherits from torch.utils.data.Dataset)\nclass ImageFolderCustom(Dataset):\n\n    # 1. Initialize with a targ_dir and transform (optional) parameter\n    def __init__(self,\n                 targ_dir: str,\n                 path_df: DataFrame,\n                 transform=None) -> None:\n\n        # Get all image paths, classes\n        self.img_df = path_df\n\n        # Set all images to proper path\n        self.img_df['path'] = self.check_path(targ_dir)\n\n        self.paths = list(self.img_df['path'])\n\n        # Setup transforms\n        self.transform = transform\n\n        self.classes, self.class_to_idx = self.find_classes()\n\n    # 2. check if its already in proper format\n    def check_path(self,\n                   targ_dir: str) -> DataFrame:\n        if str(targ_dir) in self.img_df.iloc[0,0]:\n            return self.img_df['path'].astype('string')\n        else:\n            return str(targ_dir)+ '/' +  self.img_df['path'].astype('string')\n\n    # 3. Make function to load images\n    def load_image(self,\n                   index: int) -> Image.Image:\n        \"Opens an image via a path and returns it.\"\n        image_path = self.img_df.iloc[index, 0]\n        return Image.open(image_path)\n\n    # 4. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n    def __len__(self) -> int:\n        \"Returns the total number of samples.\"\n        return self.img_df.shape[0]\n\n    # 5. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n    def __getitem__(self,\n                    index: int) -> Tuple[torch.Tensor, int]:\n        \"Returns one sample of data, data and label (X, y).\"\n        img = self.load_image(index)\n        class_name  = self.img_df.iloc[index, 1] # expects path in data_folder/class_name/image.jpeg\n        class_idx = self.class_to_idx[class_name]\n\n        # Transform if necessary\n        if self.transform:\n            return self.transform(img), class_idx # return data, label (X, y)\n        else:\n            return img, class_idx # return data, label (X, y)\n\n\n    def find_classes(self) -> Tuple[List[str], Dict[str, int]]:\n\n        col = self.img_df.columns\n        # 1. Get the class names by scanning the target directory\n        classes = sorted(self.img_df[col[1]].unique())\n\n        # 2. Raise an error if class names not found\n        if not classes:\n            raise FileNotFoundError(f\"Couldn't find any classes.\")\n\n        # 3. Create a dictionary of index labels (computers prefer numerical rather than string labels)\n        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n        return classes, class_to_idx","metadata":{"execution":{"iopub.status.busy":"2024-02-11T14:27:30.804447Z","iopub.execute_input":"2024-02-11T14:27:30.804781Z","iopub.status.idle":"2024-02-11T14:27:30.817743Z","shell.execute_reply.started":"2024-02-11T14:27:30.804751Z","shell.execute_reply":"2024-02-11T14:27:30.816856Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"manual_transforms = transforms.Compose([\n        transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n        transforms.ToTensor(), # 2. Turn image values to between 0 & 1\n        transforms.Normalize(mean = MEAN, # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n                         std = STD)\n    ])\n\n\n\n    # test_transforms = transforms.Compose([\n    #     #transforms.Resize((64, 64)),\n    #     transforms.ToTensor(),\n    # ])\n\n\ntrain_data = ImageFolderCustom(targ_dir = image_dir,\n                                          path_df = train_ds,\n                                          transform= manual_transforms)\n\ntest_data = ImageFolderCustom(targ_dir = image_dir,\n                                          path_df = test_ds,\n                                          transform= manual_transforms)\n\nclasses, class_to_idx = train_data.find_classes()","metadata":{"execution":{"iopub.status.busy":"2024-02-11T14:27:30.818866Z","iopub.execute_input":"2024-02-11T14:27:30.819123Z","iopub.status.idle":"2024-02-11T14:27:30.843751Z","shell.execute_reply.started":"2024-02-11T14:27:30.819102Z","shell.execute_reply":"2024-02-11T14:27:30.842758Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/1968766816.py:14: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self.img_df['path'] = self.check_path(targ_dir)\n","output_type":"stream"}]},{"cell_type":"code","source":"model = models.resnet50(pretrained=True).to(device)\n\n# for param in model.parameters():\n#     param.requires_grad = False\n\nmodel.fc = nn.Sequential(\n               nn.Linear(2048, 128),\n               nn.ReLU(inplace=True),\n               nn.Linear(128, 3)).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T14:27:30.845107Z","iopub.execute_input":"2024-02-11T14:27:30.845659Z","iopub.status.idle":"2024-02-11T14:27:32.318939Z","shell.execute_reply.started":"2024-02-11T14:27:30.845634Z","shell.execute_reply":"2024-02-11T14:27:32.317959Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 165MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\ndef reset_weights(m):\n  '''\n    Try resetting model weights to avoid\n    weight leakage.\n  '''\n  for layer in m.children():\n    if hasattr(layer, 'reset_parameters'):\n#         print(f'Reset trainable parameters of layer = {layer}')\n        layer.reset_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-02-11T14:27:32.321652Z","iopub.execute_input":"2024-02-11T14:27:32.322034Z","iopub.status.idle":"2024-02-11T14:27:32.327168Z","shell.execute_reply.started":"2024-02-11T14:27:32.322002Z","shell.execute_reply":"2024-02-11T14:27:32.326203Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# loss_fn = nn.CrossEntropyLoss(reduction='sum') # computes the cross entropy loss between input logits and target.\n\n# optimizer = torch.optim.Adam(model.parameters(), lr = ALPHA)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T14:27:32.330594Z","iopub.execute_input":"2024-02-11T14:27:32.330917Z","iopub.status.idle":"2024-02-11T14:27:32.339656Z","shell.execute_reply.started":"2024-02-11T14:27:32.330894Z","shell.execute_reply":"2024-02-11T14:27:32.338713Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# def train_epoch(model,device,dataloader,loss_fn,optimizer):\n#     train_loss,train_correct=0.0,0\n#     model.train()\n#     for images, labels in dataloader:\n\n#         images,labels = images.to(device),labels.to(device)\n#         optimizer.zero_grad()\n#         output = model(images)\n#         loss = loss_fn(output,labels)\n#         loss.backward()\n#         optimizer.step()\n#         train_loss += loss.item() * images.size(0)\n#         scores, predictions = torch.max(output.data, 1)\n#         train_correct += (predictions == labels).sum().item()\n\n#     return train_loss,train_correct\n\n# def valid_epoch(model,device,dataloader,loss_fn):\n#     valid_loss, val_correct = 0.0, 0\n#     model.eval()\n#     with torch.no_grad():\n#         for images, labels in dataloader:\n#             images,labels = images.to(device),labels.to(device)\n#             output = model(images)\n#             loss=loss_fn(output,labels)\n#             valid_loss+=loss.item()*images.size(0)\n#             scores, predictions = torch.max(output.data,1)\n#             val_correct+=(predictions == labels).sum().item()\n\n#     return valid_loss,val_correct","metadata":{"execution":{"iopub.status.busy":"2024-02-11T14:27:32.340628Z","iopub.execute_input":"2024-02-11T14:27:32.340920Z","iopub.status.idle":"2024-02-11T14:27:32.351517Z","shell.execute_reply.started":"2024-02-11T14:27:32.340898Z","shell.execute_reply":"2024-02-11T14:27:32.350675Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# dataset = ConcatDataset([train_data, test_data])\n# labels = [t[1] for t in dataset]","metadata":{"execution":{"iopub.status.busy":"2024-02-11T14:27:32.352593Z","iopub.execute_input":"2024-02-11T14:27:32.352880Z","iopub.status.idle":"2024-02-11T14:27:32.359464Z","shell.execute_reply.started":"2024-02-11T14:27:32.352849Z","shell.execute_reply":"2024-02-11T14:27:32.358649Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[]}\n\n\n\n\n# # Define the K-fold Cross Validator\n# kfold =  StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n#   # Start print\n# print('--------------------------------')\n\n# # K-fold Cross Validation model evaluation\n# for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset, labels)):\n#     print('Fold {}'.format(fold + 1))\n\n#     # Sample elements randomly from a given list of ids, no replacement.\n#     train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n#     test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n    \n#     # Define data loaders for training and testing data in this fold\n#     trainloader = torch.utils.data.DataLoader(\n#                       dataset, \n#                       batch_size=32, sampler=train_subsampler)\n#     testloader = torch.utils.data.DataLoader(\n#                       dataset,\n#                       batch_size=32, sampler=test_subsampler)\n    \n    \n#     model.to(device)\n#     optimizer = optim.Adam(model.parameters(), lr=0.002)\n\n#     for epoch in range(EPOCH):\n#         train_loss, train_correct=train_epoch(model,device,trainloader, loss_fn,optimizer)\n#         test_loss, test_correct=valid_epoch(model,device,testloader, loss_fn)\n\n#         train_loss = train_loss / len(trainloader.sampler)\n#         train_acc = train_correct / len(trainloader.sampler) * 100\n#         test_loss = test_loss / len(testloader.sampler)\n#         test_acc = test_correct / len(testloader.sampler) * 100\n\n#         print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %\".format(epoch + 1,\n#                                                                                                              EPOCH,\n#                                                                                                              train_loss,\n#                                                                                                              test_loss,\n#                                                                                                              train_acc,\n#                                                                                                              test_acc))\n#         history['train_loss'].append(train_loss)\n#         history['test_loss'].append(test_loss)\n#         history['train_acc'].append(train_acc)\n#         history['test_acc'].append(test_acc)  ","metadata":{"execution":{"iopub.status.busy":"2024-02-11T14:27:32.360664Z","iopub.execute_input":"2024-02-11T14:27:32.360936Z","iopub.status.idle":"2024-02-11T14:27:32.368128Z","shell.execute_reply.started":"2024-02-11T14:27:32.360914Z","shell.execute_reply":"2024-02-11T14:27:32.367172Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class KFold():\n#     wandb.init(\n#     project=\"ressnet50-v1\",\n#     config={\n#             \"epochs\": EPOCH,\n#             \"batch_size\": BATCH_SIZE,\n#             \"lr\": ALPHA,\n#             \"architecture\": \"CNN\",\n#             })\n\n    def __init__(self,\n                 model,\n                 loss_fn,\n                 optimizer,\n                 device,\n                 early_stopping = False):\n\n        self.model = model\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.device = device\n\n        self.early_stopping = early_stopping\n        self.counter = 0\n        self.early_stop = False # type: ignore\n        self.best_score = None\n\n#         # Copy your config\n#         self.config = wandb.config\n\n    def check_early_stop(self,\n                   val_loss,\n                   delta,\n                   verbose,\n                   patience,\n                   epoch):\n\n        score = -val_loss\n        # print(verbose)\n        if self.best_score is None:\n            self.best_score = score\n\n        elif score < self.best_score + delta and epoch < self.epochs:\n            self.counter += 1\n\n            if verbose:\n                print(f\"Early stopping counter: {self.counter} out of {patience}\")\n\n            if self.counter >= patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.early_stop = False\n            self.counter = 0\n            \n\n    def train_epoch(self, epoch):\n        \n        y_tr_true, y_tr_pred= [], []\n        train_loss = 0.0\n        \n        model.train()\n        for i, data in enumerate(tqdm(self.trainloader, desc=f'Epoch {epoch + 1}/{self.epochs}', unit='batch')):\n            \n            images, labels = data\n            y_tr_true.extend(labels) # collect all training labels\n            images,labels = images.to(device),labels.to(device)\n            \n            optimizer.zero_grad()\n            output = model(images)\n            loss = loss_fn(output,labels)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * images.size(0)\n            scores, predictions = torch.max(output.data, 1)\n            \n            y_tr_pred.extend(predictions.cpu())\n           \n        \n        train_correct =  accuracy_score(y_tr_true, y_tr_pred)\n\n        return train_loss,train_correct\n\n    def valid_epoch(self, epoch):\n        \n        valid_loss = 0.0\n        y_tr_true, y_tr_pred= [], []\n        \n        model.eval()\n        with torch.no_grad():\n            for i, data in enumerate(tqdm(self.testloader, desc=f'Epoch {epoch + 1}/{self.epochs}', unit='batch')):\n                \n                images, labels = data\n                y_tr_true.extend(labels) # collect all training labels\n                images,labels = images.to(device),labels.to(device)\n                \n                output = model(images)\n                loss=loss_fn(output,labels)\n                valid_loss+=loss.item()*images.size(0)\n                scores, predictions = torch.max(output.data,1)\n                y_tr_pred.extend(prediction.cpu())\n        \n        val_correct = accuracy_score(y_tr_true, y_tr_pred)\n\n        return valid_loss,val_correct\n    \n    def train(self,\n              train_data,\n              test_data,\n              epochs=1,\n              k_folds=5,\n              delta = 0,\n              patience = 10,\n              verbose = False):\n\n        self.epochs = epochs\n        self.train_dataloader = train_data\n        self.test_dataloader = test_data\n\n\n        # Create empty results dictionary\n        results = {\"epoch\":[],\n                \"train_loss\": [],\n                \"train_acc\": [],\n                \"test_loss\": [],\n                \"test_acc\": []\n        }\n\n        # Define the K-fold Cross Validator\n        kfold =  StratifiedKFold(n_splits= k_folds, shuffle=True, random_state=42)\n        \n        self.dataset = ConcatDataset([train_data, test_data])\n        labels = [t[1] for t in self.dataset]\n\n        # Make sure model on target device\n        self.model.to(self.device)\n\n        # Loop through training and testing steps for a number of epochs\n        print('--------------------------------')\n\n        # K-fold Cross Validation model evaluation\n        for fold, (train_ids, test_ids) in enumerate(kfold.split(self.dataset, labels)):\n            print('Fold {}'.format(fold + 1))\n            \n            # Sample elements randomly from a given list of ids, no replacement.\n            train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n            test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n\n            \n            # Define data loaders for training and testing data in this fold\n            self.trainloader = torch.utils.data.DataLoader(\n                              self.dataset, \n                              batch_size = BATCH_SIZE,\n                              sampler = train_subsampler)\n\n            self.testloader = torch.utils.data.DataLoader(\n                              self.dataset,\n                              batch_size = BATCH_SIZE, \n                              sampler = test_subsampler)\n\n            for epoch in range(self.epochs):\n                train_loss, train_correct = self.train_epoch(epoch)\n                test_loss, test_correct = self.valid_epoch(epoch)\n\n                train_loss = train_loss / len(self.trainloader.sampler)\n                train_acc = train_correct * 100\n                test_loss = test_loss / len(self.testloader.sampler)\n                test_acc = test_correct * 100\n                                                                                                            \n\n                # Print out what's happening\n                print(\n                f\"Epoch: {epoch+1} | \"\n                f\"train_loss: {train_loss:.4f} | \"\n                f\"train_acc: {train_acc:.4f} | \"\n                f\"test_loss: {test_loss:.4f} | \"\n                f\"test_acc: {test_acc:.4f}\"\n                )\n\n                # Update results dictionary\n                results[\"epoch\"].append(epoch+1)\n                results[\"train_loss\"].append(train_loss)\n                results[\"test_loss\"].append(test_loss)\n                results[\"train_acc\"].append(train_acc)\n                results[\"test_acc\"].append(test_acc)\n\n\n\n                if self.early_stopping:\n                    self.check_early_stop(test_loss, delta, verbose, patience, epoch)\n                    if self.early_stop:\n                        print(\"Early Stopping\")\n                        break\n\n        # Mark the run as finished\n#         wandb.finish()\n        # Return the filled results at the end of the epochs\n        return results\n","metadata":{"execution":{"iopub.status.busy":"2024-02-11T14:27:32.369653Z","iopub.execute_input":"2024-02-11T14:27:32.370034Z","iopub.status.idle":"2024-02-11T14:27:32.401855Z","shell.execute_reply.started":"2024-02-11T14:27:32.369999Z","shell.execute_reply":"2024-02-11T14:27:32.401036Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"loss_fn = nn.CrossEntropyLoss(reduction='sum') # computes the cross entropy loss between input logits and target.\n\noptimizer = torch.optim.Adam(model.parameters(), lr = ALPHA)\n\n# Start the timer\nfrom timeit import default_timer as timer\nstart_time = timer()\n\n# Setup training and save the results\nEngine = KFold(model=model, loss_fn=loss_fn, optimizer=optimizer, device=device, early_stopping=True)\nhistory = Engine.train(train_data=train_data, test_data=test_data, epochs=EPOCH, verbose=True, patience = 5)\n\n# End the timer and print out how long it took\nend_time = timer()\nprint(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-02-11T14:27:32.402999Z","iopub.execute_input":"2024-02-11T14:27:32.405986Z","iopub.status.idle":"2024-02-11T14:29:28.654257Z","shell.execute_reply.started":"2024-02-11T14:27:32.405951Z","shell.execute_reply":"2024-02-11T14:29:28.652686Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"--------------------------------\nFold 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/10:   0%|          | 0/95 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d731e8e41bcf405c917a716e2e9ba953"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Setup training and save the results\u001b[39;00m\n\u001b[1;32m     10\u001b[0m Engine \u001b[38;5;241m=\u001b[39m KFold(model\u001b[38;5;241m=\u001b[39mmodel, loss_fn\u001b[38;5;241m=\u001b[39mloss_fn, optimizer\u001b[38;5;241m=\u001b[39moptimizer, device\u001b[38;5;241m=\u001b[39mdevice, early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# End the timer and print out how long it took\u001b[39;00m\n\u001b[1;32m     14\u001b[0m end_time \u001b[38;5;241m=\u001b[39m timer()\n","Cell \u001b[0;32mIn[14], line 162\u001b[0m, in \u001b[0;36mKFold.train\u001b[0;34m(self, train_data, test_data, epochs, k_folds, delta, patience, verbose)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtestloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m    157\u001b[0m                   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset,\n\u001b[1;32m    158\u001b[0m                   batch_size \u001b[38;5;241m=\u001b[39m BATCH_SIZE, \n\u001b[1;32m    159\u001b[0m                   sampler \u001b[38;5;241m=\u001b[39m test_subsampler)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m--> 162\u001b[0m     train_loss, train_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     test_loss, test_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_epoch(epoch)\n\u001b[1;32m    165\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainloader\u001b[38;5;241m.\u001b[39msampler)\n","Cell \u001b[0;32mIn[14], line 80\u001b[0m, in \u001b[0;36mKFold.train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     75\u001b[0m     scores, predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     77\u001b[0m     y_tr_pred\u001b[38;5;241m.\u001b[39mextend(predictions)\n\u001b[0;32m---> 80\u001b[0m train_correct \u001b[38;5;241m=\u001b[39m  \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_tr_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_loss,train_correct\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:192\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m validate_parameter_constraints(\n\u001b[1;32m    188\u001b[0m     parameter_constraints, params, caller_name\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\n\u001b[1;32m    189\u001b[0m )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    202\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:221\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:88\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     86\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m     87\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 88\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_pred\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m y_type \u001b[38;5;241m=\u001b[39m {type_true, type_pred}\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/multiclass.py:309\u001b[0m, in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse_pandas:\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my cannot be class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseSeries\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseArray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_multilabel\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# DeprecationWarning will be replaced by ValueError, see NEP 34\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# We therefore catch both deprecation (NumPy < 1.24) warning and\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# value error (NumPy >= 1.24).\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/multiclass.py:169\u001b[0m, in \u001b[0;36mis_multilabel\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    167\u001b[0m warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mVisibleDeprecationWarning)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_y_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (np\u001b[38;5;241m.\u001b[39mVisibleDeprecationWarning, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    883\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    182\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(array)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.array_api\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:1030\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1030\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."],"ename":"TypeError","evalue":"can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.","output_type":"error"}]},{"cell_type":"code","source":"\navg_train_loss = np.mean(history['train_loss'])\navg_test_loss = np.mean(history['test_loss'])\navg_train_acc = np.mean(history['train_acc'])\navg_test_acc = np.mean(history['test_acc'])\n\nprint('Performance of {} fold cross validation'.format(K_FOLDS))\nprint(\"Average Training Loss: {:.4f} \\t Average Test Loss: {:.4f} \\t Average Training Acc: {:.3f} \\t Average Test Acc: {:.3f}\".format(avg_train_loss,avg_test_loss,avg_train_acc,avg_test_acc)) ","metadata":{"execution":{"iopub.status.busy":"2024-02-11T14:29:28.655346Z","iopub.status.idle":"2024-02-11T14:29:28.655794Z","shell.execute_reply.started":"2024-02-11T14:29:28.655571Z","shell.execute_reply":"2024-02-11T14:29:28.655589Z"},"trusted":true},"execution_count":null,"outputs":[]}]}